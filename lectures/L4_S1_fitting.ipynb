{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12cb1e4f",
   "metadata": {},
   "source": [
    "# Fitting or: how I learned to stop worrying and make that darng model pass through my data-points\n",
    "\n",
    "> **(First question: do you get the reference?)**\n",
    "\n",
    "We have obtained (either because we have measured it or because it has been given to us) a **SUMMARY STATISTICS** of our population of objects.\n",
    "\n",
    "We are scientists and we want to represent summary statistics with analytical laws:\n",
    "\n",
    "- because those can be made data-set independent and passed around more easily\n",
    "- because a summary statistics measured on some population can be compared to that measured on some other population\n",
    "- because they can give us physical insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc06ff02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy, os\n",
    "import matplotlib.pyplot as plt\n",
    "basedir = '../datasets'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "537fdf9c",
   "metadata": {},
   "source": [
    "Let's load the dataset we have stored before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ef32e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "infile = os.path.join(basedir, 'SFR_density_function.npz')\n",
    "infile"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91259dd9",
   "metadata": {},
   "source": [
    "This to check that the file exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc3382c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls $infile"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39db47cf",
   "metadata": {},
   "source": [
    "It's an ``npz`` file so I can load it directly with numpy using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "172c1848",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = numpy.load(infile) # remember this uses the pickle protocol by default"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4f49b15",
   "metadata": {},
   "source": [
    "**WHY IS IT OK TO PASS AROUND STUFF WITH PICKLE PROTOCOL HERE?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bea718f3",
   "metadata": {},
   "source": [
    "I can check what's is stored inside"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0efbc29f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a8e8f7e",
   "metadata": {},
   "source": [
    "And I can unpack the values tuple\n",
    "\n",
    "(as much as I would have done with a dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c89d0e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "lSFR, nSFR, nSFR_e = data.values()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79376733",
   "metadata": {},
   "source": [
    "And we can reproduce here the previous plot just for checking everything looks as expected:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90756482",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1)\n",
    "_ = ax.set(\n",
    "    xscale='log', yscale='log',\n",
    "    xlabel='$\\\\log [\\\\mathrm{SFR}/(M_\\\\star\\\\cdot\\\\mathrm{yr}^{-1})]$',\n",
    "    ylabel='$\\\\log n[\\\\mathrm{SFR}/(M_\\\\star\\\\cdot\\\\mathrm{yr}^{-1}\\\\cdot Mpc^{-3})]$'\n",
    ")\n",
    "_ = ax.errorbar(10**lSFR, nSFR, yerr=nSFR_e, \n",
    "                marker='o', linestyle='none', color='k', label='data')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fae12942",
   "metadata": {},
   "source": [
    "### Guess what? This is a Schechter function\n",
    "\n",
    "$$n(x) = \\mathcal{N} \\left(\\dfrac{x}{c}\\right)^{(1-\\alpha)} \\exp\\left(-\\dfrac{x}{c}\\right)$$\n",
    "\n",
    "which it's basically a power-law with an exponential cut-off.\n",
    "**Give me some examples of Schechter-like functions please.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e9101ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def schechter_function ( lx, a = 1.5, n = 0.01, c = 1.0 ) :\n",
    "    \"\"\"Schechter-like function\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    x : scalar or sequence\n",
    "        log10 values\n",
    "    a : scalar\n",
    "        alpha index of the power-low\n",
    "    n : scalar\n",
    "        normalization\n",
    "    c : scalar\n",
    "        critical value\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    : scalar or sequence\n",
    "        schechter function computed per each grid point\n",
    "        for given set of parameters\n",
    "    \"\"\"\n",
    "    y = 10**(numpy.array( lx ) - c)\n",
    "    return n * y**(1-a) * numpy.exp( - y )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08c70865",
   "metadata": {},
   "source": [
    "This depends on **3 PARAMETERS**: ``a``, ``n``, ``c``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "804a4077",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1)\n",
    "_ = ax.set(\n",
    "    xscale='log', yscale='log',\n",
    "    xlabel='$\\\\log [\\\\mathrm{SFR}/(M_\\\\star\\\\cdot\\\\mathrm{yr}^{-1})]$',\n",
    "    ylabel='$\\\\log n[\\\\mathrm{SFR}/(M_\\\\star\\\\cdot\\\\mathrm{yr}^{-1}\\\\cdot Mpc^{-3})]$'\n",
    ")\n",
    "_ = ax.plot(10**lSFR, schechter_function(lSFR), ls='--', color='gray', label='model')\n",
    "_ = ax.errorbar(10**lSFR, nSFR, yerr=nSFR_e, \n",
    "                marker='o', linestyle='none', color='k', label='data')\n",
    "_ = ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c46ecc8a",
   "metadata": {},
   "source": [
    "### EXERCISE: How to quantify your confidence in the model?\n",
    "\n",
    "For simplicity let's assume that the errors are Gaussian (which is a reasonable safe assumption)\n",
    "\n",
    "(even though we have built them as Poissonian, but we are going to ignore this)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32cb1afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write here the estimator\n",
    "def confidence ( data, error, model ) :\n",
    "    data = numpy.asarray(data)\n",
    "    error = numpy.asarray(error)\n",
    "    model = numpy.asarray(model)\n",
    "    return (\n",
    "        (( data - model )/error)**2\n",
    "    ).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1918b590",
   "metadata": {},
   "outputs": [],
   "source": [
    "confidence( nSFR, nSFR_e, schechter_function(lSFR) )/(nSFR.size-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4e75be0",
   "metadata": {},
   "source": [
    "### EXERCISE: Now what?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d08d74a",
   "metadata": {},
   "outputs": [],
   "source": [
    "help(schechter_function)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fba71cbd",
   "metadata": {},
   "source": [
    "#### Grid methods\n",
    "\n",
    "Compute a set of models on a grid in the parameter space.\n",
    "Here the parameters are 3, therefore the parameter space i 3-dimensional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbae17da",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = numpy.random.default_rng(seed=555)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e121924a",
   "metadata": {},
   "source": [
    "Instead of making an actual grid, with the risk that the number of elements blows up with the growth of the parameter space dimension, we are instead uniformly sampling a box, in the parameter space, assuming some limits on each parameter's dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "740b534b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pgrid = rng.uniform( \n",
    "    low=[0.,0.,-2], \n",
    "    high=[3, 3, 1], \n",
    "    size=(1024**2,3) \n",
    ")\n",
    "pgrid.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "019fef22",
   "metadata": {},
   "source": [
    "With the command above I have generated a matrix, the first 4 lines look like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f456370a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pgrid[:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47a9626a",
   "metadata": {},
   "source": [
    "Where the first column corresponds to the first parameter (``a``) and so on.\n",
    "\n",
    "I can pass these values to the ``schechter_function`` model we have defined above, modulo that I smartly organize them in a way that allows NumPy to **broadcast** the function across the whole $26\\times1024^2$ space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14f8431c",
   "metadata": {},
   "source": [
    "i.e. ``pgrid.T`` transposes the original matrix, so that now it is a $3\\times1024^2$ matrix, where the 3 lines contain $1024^2$ random samples each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e82402",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "models_grid = schechter_function(\n",
    "    lSFR, \n",
    "    pgrid.T[0][:,numpy.newaxis], \n",
    "    pgrid.T[1][:,numpy.newaxis], \n",
    "    pgrid.T[2][:,numpy.newaxis]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e699f073",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_grid.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e25b75ee",
   "metadata": {},
   "source": [
    "The ``models`` matrix contains $1024^2$ estimate of the model, one per each random position in the parameter space.\n",
    "\n",
    "To measure the $\\chi^2$ of these models exploiting broadcasting, we have to slightly modify the function we have defined above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1fbf384",
   "metadata": {},
   "outputs": [],
   "source": [
    "def confidence ( data, error, model ) :\n",
    "    data = numpy.asarray(data)\n",
    "    error = numpy.asarray(error)\n",
    "    model = numpy.asarray(model)\n",
    "    return (\n",
    "        (( data - model )/error)**2\n",
    "    ).sum( axis = -1 ) # <----------- Here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f1f3e58",
   "metadata": {},
   "source": [
    "The argument I have passed to the ``sum`` function, i.e. ``axis=-1``, tells the function to perfom the sum of the elements only on the last axis.\n",
    "\n",
    "- if the above function is called on a single model, nothing changes from the behaviour of before, the sum is done for the 26 data-points\n",
    "- if instead the function is called on our $1024^2\\times26$ grid, it is done only on the last dimension, providing us with $1024^2$ values of $\\chi^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b4f218d",
   "metadata": {},
   "outputs": [],
   "source": [
    "chi2_grid = confidence( nSFR, nSFR_e, models_grid )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eca1dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "chi2_grid.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "425d6706",
   "metadata": {},
   "source": [
    "We can assign the index of the smallest $\\chi^2$ value found to some variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72cf7ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "ibest_grid = chi2_grid.argmin()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3fe3496",
   "metadata": {},
   "source": [
    "the above index tells me that ``models[ibest]`` is the best-fitting model of my sample, with a reduced $\\chi^2$ value of"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c0b33df",
   "metadata": {},
   "outputs": [],
   "source": [
    "chi2_grid.min()/(nSFR.size-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c0ba62e",
   "metadata": {},
   "source": [
    "So let's plot the new model and also compare the standardised residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f931e291",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2,1, sharex=True, gridspec_kw={'height_ratios':(2,1), 'hspace':0.0})\n",
    "\n",
    "# first sub-plot\n",
    "_ = axs[0].set(\n",
    "    xscale='log', yscale='log',\n",
    "    ylabel='$\\\\log n[\\\\mathrm{SFR}/(M_\\\\star\\\\cdot\\\\mathrm{yr}^{-1}\\\\cdot Mpc^{-3})]$'\n",
    ")\n",
    "_ = axs[0].plot(10**lSFR, schechter_function(lSFR), ls='--', color='gray', label='model naive')\n",
    "_ = axs[0].plot(10**lSFR, models_grid[ibest_grid], ls='-', color='orange', label='model grid')\n",
    "_ = axs[0].errorbar(10**lSFR, nSFR, yerr=nSFR_e, \n",
    "                marker='o', linestyle='none', color='k', label='data')\n",
    "_ = axs[0].legend()\n",
    "\n",
    "# second sub-plot\n",
    "_ = axs[1].set(\n",
    "    xscale='log',\n",
    "    xlabel='$\\\\log [\\\\mathrm{SFR}/(M_\\\\star\\\\cdot\\\\mathrm{yr}^{-1})]$',    \n",
    "    ylabel='$\\chi$'\n",
    ")\n",
    "_ = axs[1].axhline(0.0, color='orange', ls='-')\n",
    "_ = axs[1].plot(10**lSFR, (nSFR - models_grid[ibest_grid])/nSFR_e, marker='o', color='k', ls='none')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd40b784",
   "metadata": {},
   "source": [
    "Standardised residuals are defined as\n",
    "\n",
    "$$\\chi_i = \\dfrac{\\text{data} - \\text{model}}{\\text{error}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "297fd272",
   "metadata": {},
   "source": [
    "#### SciPy's minimization algorithms\n",
    "[``scipy.optimize.minimize``](https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize.html)\n",
    "\n",
    "Can probably be classified as a grid algorithm? I don't know actually.\n",
    "\n",
    "Anyways, these have implemented smarter algorithms w.r.t. what we did above..\n",
    "\n",
    "When to use them?\n",
    "\n",
    "- some people's answer is \"always\" (but we do not like this answer)\n",
    "- **the parameter space is small** (i.e. the **size of the prior** is small)\n",
    "- when we have an unknown dataset and we want some quick test to perform, modulo that:\n",
    "    * the dataset is small\n",
    "    * the parameter space is small\n",
    "- when we don't know what are reasonable limits for our grid (some of these minimization algorithms can be run without constraining the parameter space but using an initial guess instead)\n",
    "\n",
    "> **NOTE** Those listed above are **rules of thumb**, there are exceptions that you will learn to spot with experience.\n",
    "\n",
    "> **NOTE ALSO** The above arguments also apply to the naive grid method we have seen before\n",
    "\n",
    "Also, there are very sofisticated algorithms implemented in ``minimize``, such as algorithms that exploit Hessians and Jacobians of the objective function. Therefore in some cases (when such functions can be computed easily, these methods could be quite helpful).\n",
    "\n",
    "\n",
    "> A small digression on *Bayesianity* of your statistical analysis:\n",
    ">\n",
    "> **when is it possible to say that what we did is Bayesian?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b2b07c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7402b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "help(minimize)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d51aef1b",
   "metadata": {},
   "source": [
    "We need an *objective function*, what does it mean?\n",
    "\n",
    "It means that we need a function, depending on the free parameters of the model, that has to be minimized.\n",
    "\n",
    "- the schechter function is the one that depends on the parameters, but is not what we need to minimize\n",
    "- the $\\chi^2$ is the function we need to minimize, but with our definition above it does not depend directly on the parameters\n",
    "\n",
    "so we just define a new function which is the combination of the ones above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd682318",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_function ( p, x, y, e ) :\n",
    "    return confidence(\n",
    "        y, e, \n",
    "        schechter_function( x, *p )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "198a5a9b",
   "metadata": {},
   "source": [
    "Than ``minimize`` needs a starting point, we are going to use the default values of the parameters of ``schechter_function``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb1d1d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "p0 = [1.5, 0.01, 1.0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d895ede6",
   "metadata": {},
   "source": [
    "And that's it, we are ready to run minimize, note though that ``objective_function`` requires some additional arguments besides the free parameters, we can pass them as a tuple of ``args``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d8a0d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time res = minimize( objective_function, [1.5, 0.01, 1.0], args = (lSFR, nSFR, nSFR_e) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ac7a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1b968ef",
   "metadata": {},
   "source": [
    "We can check also in this case the reduced $\\chi^2$ value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d19e2a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "confidence( nSFR, nSFR_e, schechter_function(lSFR, *res.x)) / (nSFR.size-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23879449",
   "metadata": {},
   "source": [
    "For reference, with the grid method we got"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0bdb7d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "chi2_grid[ibest_grid]/ (nSFR.size-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0683030a",
   "metadata": {},
   "source": [
    "And we can plot the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee604acb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2,1, sharex=True, gridspec_kw={'height_ratios':(2,1), 'hspace':0.0})\n",
    "\n",
    "# first sub-plot\n",
    "_ = axs[0].set(\n",
    "    xscale='log', yscale='log',\n",
    "    ylabel='$\\\\log n[\\\\mathrm{SFR}/(M_\\\\star\\\\cdot\\\\mathrm{yr}^{-1}\\\\cdot Mpc^{-3})]$'\n",
    ")\n",
    "_ = axs[0].plot(10**lSFR, schechter_function(lSFR), ls='--', color='gray', label='model naive')\n",
    "_ = axs[0].plot(10**lSFR, models_grid[ibest_grid], ls='-', color='orange', label='model grid')\n",
    "_ = axs[0].plot(10**lSFR, schechter_function(lSFR, *res.x), ls='-', color='green', label='model scipy')\n",
    "_ = axs[0].errorbar(10**lSFR, nSFR, yerr=nSFR_e, \n",
    "                marker='o', linestyle='none', color='k', label='data')\n",
    "_ = axs[0].legend()\n",
    "\n",
    "# second sub-plot\n",
    "_ = axs[1].set(\n",
    "    xscale='log',\n",
    "    xlabel='$\\\\log [\\\\mathrm{SFR}/(M_\\\\star\\\\cdot\\\\mathrm{yr}^{-1})]$',    \n",
    "    ylabel='$\\chi$'\n",
    ")\n",
    "_ = axs[1].axhline(0.0, ls='-', color='green')\n",
    "_ = axs[1].plot(10**lSFR, (nSFR - schechter_function(lSFR, *res.x))/nSFR_e, marker='o', color='k', ls='none')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df2c4a4d",
   "metadata": {},
   "source": [
    "#### Monte-Carlo Markov Chain (MCMC) methods\n",
    "\n",
    "We will use [``emcee``](https://emcee.readthedocs.io/en/stable/), but I will give an explanation of what a markov-chain **AT THE BLACKBOARD**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e123304f",
   "metadata": {},
   "source": [
    "First of all, instead of minimizing the $\\chi^2$ we will **MAXIMIZE THE POSTERIOR PROBABILITY**.\n",
    "\n",
    "**QUESTION:** Therefore, what are we going to work with?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8eab072",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_likelihood ( p, x, y, e ) :\n",
    "    \n",
    "    a, ln, c = p\n",
    "    n = 10.**ln\n",
    "    m = schechter_function( x, a, ln, c )\n",
    "    chi2 = confidence( y, e, m )\n",
    "    \n",
    "    return - 0.5 * chi2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a29c101c",
   "metadata": {},
   "source": [
    "It is good practice to always test a function when we define it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abdfdf1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_likelihood( p0, lSFR, nSFR, nSFR_e )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e58c7dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_prior ( p, lims ) :\n",
    "    \n",
    "    p = numpy.asarray(p)\n",
    "    lims = numpy.asarray(lims)\n",
    "    \n",
    "    if numpy.all( (lims[0] < p)&(p < lims[1]) ) :\n",
    "        return 0.0\n",
    "    return -numpy.inf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f497b47a",
   "metadata": {},
   "outputs": [],
   "source": [
    "lims = numpy.array([[1.,-1.,-1], \n",
    "                    [2., +1, 2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca4310a",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_prior( p0, lims )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a2760d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_prob ( p, x, y, e, lims ) :\n",
    "    \n",
    "    # compute the prior\n",
    "    lp = log_prior( p, lims )\n",
    "    if not numpy.isfinite( lp ) :\n",
    "        return -numpy.inf\n",
    "    \n",
    "    # return the log( likelihood * prior ) = log(likelihood) + log(prior)\n",
    "    return log_likelihood( p, x, y, e ) + lp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "952a51a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_prob( p0, lSFR, nSFR, nSFR_e, lims ) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c6c8b3b",
   "metadata": {},
   "source": [
    "We are all set up to run our first MCMC sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d8d05c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ndim, nwalkers = 3, 16\n",
    "pstart = rng.uniform( \n",
    "    low =lims[0], \n",
    "    high=lims[1], \n",
    "    size=(nwalkers, ndim),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7add5e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pstart.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1dc2e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import emcee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b61e98d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler = emcee.EnsembleSampler(nwalkers, ndim, log_prob, args=(lSFR, nSFR, nSFR_e, lims))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bd84deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "state = sampler.run_mcmc(pstart, 8*1024, progress = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6a8f6c9",
   "metadata": {},
   "source": [
    "How do we extract information on this run now?\n",
    "\n",
    "It is an MCMC therefore samples are drawn, then accepted or discarded. We can have a first diagnostic by checking what was the acceptance fraction of this run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c85b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler.acceptance_fraction.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6678d291",
   "metadata": {},
   "source": [
    "typically, values $\\gtrsim 0.3\\div0.4 \\equiv 30\\div40\\%$ are symptom of a good run. So everything's fine up to here.\n",
    "\n",
    "> Note that all the values returned by the ``sampler`` object are arrays or matrices with one of the dimension equal to the number of walkers, i.e. all walkers are treated separately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c29885",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = sampler.get_chain(flat=True) # <--- flat=True means we are flattening on the walkers dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d024d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3474b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "970ca318",
   "metadata": {},
   "source": [
    "Each element of this ``samples`` matrix is a position in the parameter space, for which an estimate of the posterior probability has been computed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ee6a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "loglike = sampler.get_log_prob(flat=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23bd2669",
   "metadata": {},
   "source": [
    "We can retrieve the best-fitting value by finding where ``logprob`` (i.e. the **log-likelihood**) was maximum:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d41d79d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ibest_mcmc = loglike.argmax()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "932edd94",
   "metadata": {},
   "source": [
    "We have to find, among the samples, the position in the parameter space where the maximum a-posteriori probability has been found:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c976311b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pbest = samples[ibest_mcmc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "978534c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pbest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "152563df",
   "metadata": {},
   "source": [
    "And that's it! We got our best-fitting parameters, we can therefore, and once again, compute the $\\chi^2$ and plot the resulting best-fitting model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb58b929",
   "metadata": {},
   "outputs": [],
   "source": [
    "confidence(nSFR, nSFR_e, schechter_function(lSFR, *pbest))/(nSFR.size-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c302ff4",
   "metadata": {},
   "source": [
    "Which is similar to the value found with the SciPy's algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e6c6bec",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2,1, sharex=True, gridspec_kw={'height_ratios':(2,1), 'hspace':0.0})\n",
    "\n",
    "# first sub-plot\n",
    "_ = axs[0].set(\n",
    "    xscale='log', yscale='log',\n",
    "    ylabel='$\\\\log n[\\\\mathrm{SFR}/(M_\\\\star\\\\cdot\\\\mathrm{yr}^{-1}\\\\cdot Mpc^{-3})]$'\n",
    ")\n",
    "_ = axs[0].plot(10**lSFR, schechter_function(lSFR), ls='--', color='gray', label='model naive')\n",
    "_ = axs[0].plot(10**lSFR, models_grid[ibest_grid], ls='-', color='orange', label='model grid')\n",
    "_ = axs[0].plot(10**lSFR, schechter_function(lSFR, *res.x), ls='-', color='green', label='model scipy')\n",
    "_ = axs[0].plot(10**lSFR, schechter_function(lSFR, *pbest), ls='-', color='red', label='model emcee')\n",
    "_ = axs[0].errorbar(10**lSFR, nSFR, yerr=nSFR_e, \n",
    "                marker='o', linestyle='none', color='k', label='data')\n",
    "_ = axs[0].legend()\n",
    "\n",
    "# second sub-plot\n",
    "_ = axs[1].set(\n",
    "    xscale='log',\n",
    "    xlabel='$\\\\log [\\\\mathrm{SFR}/(M_\\\\star\\\\cdot\\\\mathrm{yr}^{-1})]$',    \n",
    "    ylabel='$\\chi$'\n",
    ")\n",
    "_ = axs[1].axhline(0.0, ls='-', color='red')\n",
    "_ = axs[1].plot(10**lSFR, (nSFR - schechter_function(lSFR, *pbest))/nSFR_e, marker='o', color='k', ls='none')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c3fd993",
   "metadata": {},
   "source": [
    "How do I check whether my MCMC run has converged?\n",
    "We can first check how the likelihood has varied in time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d98c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loglike[1024::128])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d5a1a9a",
   "metadata": {},
   "source": [
    "This is already a good symptom, it is telling us that, after some steps, the value of the likelihood converged and did not change much.\n",
    "\n",
    "> Note that I have sub-sampled the array and cut out some initial steps (which are called **BURN-IN**)\n",
    "\n",
    "There are further diagnostics to evaluate your run, these are listed in the [``emcee`` documentation](https://emcee.readthedocs.io/en/stable/tutorials/autocorr/) and usually they depend on the sampling algorithm (and library) you are using."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4fd213b",
   "metadata": {},
   "source": [
    "Last but not least, since we have run a parameter space sampling, we can see the posterior probability distributed on the parameter space. Doing this plot starting from ``matplotlib`` functions is possible but hard, nobody does it, people uses instead libraries designed for that.\n",
    "\n",
    "One of these libraries is [``Â¢orner``](https://corner.readthedocs.io/en/latest/)\n",
    "\n",
    "We are using instead [GetDist](https://getdist.readthedocs.io/en/latest/index.html) because I am more familiar with it and it has a nice [plot gallery](https://getdist.readthedocs.io/en/latest/plot_gallery.html) from where to fish the plot that better suits your needs.\n",
    "\n",
    "You can choose your favourite one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "956fbbe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from getdist import plots, MCSamples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0548ad8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "mcsamples = MCSamples(\n",
    "    samples = samples,\n",
    "    loglikes = -loglike,\n",
    "    names = ['a', 'logN', 'c'], \n",
    "    sampler = 'mcmc',\n",
    "    settings = { \n",
    "        'mult_bias_correction_order' : 1,\n",
    "        'smooth_scale_2D' : .5, \n",
    "        'smooth_scale_1D' : .5, \n",
    "        'fine_bins': 512,\n",
    "        'fine_bins_2D' : 512,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c29965f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = plots.get_subplot_plotter()\n",
    "g.triangle_plot(\n",
    "    [mcsamples], \n",
    "    filled=True, title_limit=1\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
